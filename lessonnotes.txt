Kubernetes Lesson Notes

Part 1 Main Kubernetes Components:

* Pod - Contains Containers, which have your apps running inside them, (maybe DB too, in a seperate container). Usually one main application per Pod.
So one applicaiton, one db maybe, in one pod. Each pod gets it's OWN IP address, NOT THE CONTAINER. Each pod can communicate with each other through these 
virtual IPs. Pods can DIE very easily...that's okay. If the pod dies, a new one will get created in it's place...which will get assigned a new IP address.
Because pods live and die all the time, you need to connect DBs to them through a 'Service'.

* Service is a permanant IP address that can be attached to each pod. Even if the Pod dies, the Service and it's IP address will stay. 
It is also a Load Balancer which sends request to different avaliable Nodes/Pods.

*External Service - An external service exposes the app to external sources,(like the web, for example). You'db
want your URL to look like THIS, for production: https://my-app.com. An IP Address is good for testing tho

*Internal Service - A service exposed INTERNALLY to communicate within your app, (like for DBs which you shouldn't be exposing 
trhough the web).

* Ingress - For an app name that would be in contact with the External service, you would create an 'Ingress'. The request STARTS
at the Ingress, then gets transferred as need be to a service.

Setup/Configuration.

*Database URL is usually put into the build applicaiton. This would have to be rebuilt int docker and pushed to a hub or somewhere.

*Kubernetes has a tool called ConfigMap, which handles external configuration of your application. COntains config data like URL of database, for example.
You would just need to connect it to a pod. That way, you don't have to build a new image, just change the ConfigMap.
DON'T PUT CREDENTIALS INTO CONFIGMAP. PUT IT IN SECRET.

* Secret is like configmap but for secret credentials like passwords. IT's base64 encoded. Warning, it's not enabled by default!

* Volumes - attaches a physical hard drive storage to your pod,(ideally to store DB information). Could be local or remote.
Think of it as an external hard drive into the K8 cluster. K8 does NOT manage data persistance.

* Node - A server connected to services with a managed IP. These contain pods that run our application/DB.
Each node has multipe pods on them.

* Deployment - A Blueprint for 'MyApp' pods. (Actually, this is an abstraction ON TOP of the Pods).
These will be managed to have a set amount active/deployed at one time.
In practice, you don't work with Pods...you work with deployments. You can scale the amount of replicas up and down.
DB cannot be replicated in a deployment...because DB have a state. For that, you need a 'StatefulSet''

* StatefulSet - Used for applications like DBs, (MySQL, etc.). Used for persistant data and scaling, making 
sure the DB reads/writes are syncrhonized. These deployments are somewhat tedious. That's why
DB are reccommended to be hosted OUTSIDE of the Kubernets cluster.


Part 2 K8 Architechture

* 3 processes must be installed on every node.
Container runtime, (like docker). 

Kubelet, which is a kubernetes process that interfaces with the running containers AND the node.
Kubelet actually manages starting the pod with a container inside.

Kube Proxy forwards the requests from services to the pods. For instance, it will take a request from the app
on the SAME pod, and forward it to a DB on the SAME pod.

* Master Nodes - Manage all the Application Nodes beneath them. Must have 4 processes running:

API Server - Like a Cluster Gateway of any updates into the cluster or queries intot he cluster. Acts as a gatekeeper for 
authentication. You have to talk to the API server on the master node. It validates your request, then forwards it to other processes,
(for creating a pod or whatever, or querying the health of your cluster). Good cuz you only have 1 entrypoint.

Scheduler - Schedules new pods inside the worker nodes. It also knows WHERE to put the pods, in terms of resources.  Kubelet is the actually 
thing that starts it.

Controller Manager - Detects when nodes die,(state changes in genearl). Makes a request to the scheduler to do that.

etcd - A Key value store of a cluster state...the cluster brain. If things change in the cluster, it gets stored/logged. For example, scheduler and 
api server use the information stored in here to perform actions. Applicaiton data is NOT stored in here, just for the master node processes data to work.

Master nodes are usually duplicated to control all of this.

Example Cluster setup: 2 Master Nodes, 3 work nodes.
Master nodes need less resources, worker nodes need more.Nodes can scale horizationtally as needed.

Part 3 Minkube Setup and Kubectl - Local Setup

Production Setup is 2 Master Nodes, on their on Physical/Virtual Nodes. Also, 4 or so Worker Nodes, also on their own VMs or Phsycial servers.
For testing on local machine, there's MiniKube

* MiniKube -  On Node K8 setup, with a 'Master Process' and a 'Worker Process' running. Docker is pre-installed, so pods can be installed. It is used in a virtual box or hypervisor.
Used primarily for testing.

* Kubectl - Used for interacting with MiniKube clusters. Can run commands to work with services, creates pods, create nodes, etc. Interacts with the 
API server on master node. You can also do this with your own API that accesses the Master API Server, or through a UI.

MiniKube Setup: https://minikube.sigs.k8s.io/docs/start/
KubeCTL Setup: https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

minikube start starts a new cluster
NOTE, you need a hypervisor to start all of this. If you want a specific hypervisor you installed,
(in the tutorial, she used hyperkit), you can type this command:
minikube start --vm-driver=hyperkit
For Docker: minikube start --driver=docker
Make Docker the Default Driver: minikube config set driver docker

* Note, you'll need to do this every time your host machine shuts down.

Some commands: 
kubectl get nodes = gets all your nodes.
minikube status = gets the status of your minikube
kubectl version = latest version of kubernetes installed



Part 4 Basic Kubectl Commands
kubectl get nodes
kubectl get pods
kubectl get services

To create pods:
kubectl create

* In practice, YOU aren't working with the pods, they are the smallest unit. YOU are working with deployments, 
the abstraction OVER pods.

Usage for creating things:
kubectl create deployment NAME --image=image [--dry-run] [options]

Example nginx deployment:
kubectl create deployment nginx-depl --image=nginx
										name o pod         the image
deployment has all the info for creating the pod

kubectl get replicaset gets the replicaset for the pods created...you reallly won't need to mess with these, just with deployments.

Layers of Abstraction:
 - Deployment manages a Replica-Set
	-Replicaset manage a pod
	- Pod is an abstraction of a container,(Docker-Image)

kubectl edit deployment NAME-OF-DEPLOYMENT
This generates a configuration file with default vides...she has a seperate video to break this down.

Do this to change the default text editor: https://www.youtube.com/watch?v=W3ZL5ehG8BA
example: export KUBE_EDITOR="code --wait"
verify: echo $KUBE_EDITOR

After editing this, a new pod will be created, old pod will die with new image.

To view logs inside the application pods:
kubectl logs POD_NAME

For her example, we created another deployment:
kubectl create deployment mongo-depl --image=mongo

You can also use 'describe' to get additional information about the pod:
kubectl describe pod POD_NAME

Another debugging tool is kubectl exec, which gets the actual terminal of the container:
Example: kubectl exec -it mongo-depl-5fd6b7d4b4-2zhct -- bin/bash

For deleting pods:
kubectl delete deployment 	NAME_O_DEPLOYMENT

All this command line crap can be hard to manage...that's why most people manage this through the Kubectl config files
Example: kubectl apply -f [file-name]          So this takes the configuration file to edit.
our example: kubectl apply -f nginx-deployment.yaml
creating that file: touch nginx-deployment.yaml
If you run that command again after making a change, it wont' create a new deployment, 
just update the already created deployment with that configuartion file


Part 5 Configuartion file for Deployments
Each Configuartion file has 3 parts: 

Extra: At the top, it's what type of 'item' you're creating. Example:
apiVersion: v1  === what version you're deploying in.
kind: Service === what you are configuring the file for
You can google this stuff for the different K8 components to see what their config yaml values are and what they do.

-- Part 1 - Metadata : Where the Meteadaata is, like the name.

-- Part 2 - Specification : Every configuation you want to apply for that component

The different Attributes are different based on what deployment you're configuring,(service, for example)

-- Part 3 - Status : This is automatically generated by  Kubernetes. It takes what's in the config yaml file, vesrus
what is ACTUALLY RUNNING IN KUBERNETES and notes something to fix if it dosen't match what the config file specified.
For example, if you have TWO replicas specified in a config file, and Kubernetes detects only 1 at runtime, Kubernetes will
update the state continuously

Kubernetes gets this status data from the etcd...THE BRAIN that gets updated from other master components.

To see if your yaml is valid, you can google 'yaml validator'
Good practice is to store these config files in your application code...'infrastructure as a code'...or you can have your own github for this configuration.

Lot to see in this section for configuration...might need to watch this part again. 

For services to forward incoming requests to application pods, the 'targetPort' must match the 'containerPort'
Ran these again to update our config files and start service specification.
kubectl apply -f nginx-deployment.yaml
kubectl apply -f nginx-service.yaml
kubectl describe service nginx-service

You can get more detailed pod information,(including IP Address) with this: kubectl get pod -o wide

Get status of cluster in etcd:
kubectl get deployment nginx-deployment -o yaml
Next, save that to a file
kubectl get deployment nginx-deployment -o yaml > nginx-deployment-result.yaml
You can take a look at this and it helps with debugging

You can delete the services as follows: 
kubectl delete -f nginx-deployment.yaml
kubectl delete -f nginx-service.yaml



Part 6 Complete Demo Project
What's it look like: Internal Service, an External Service, Secrets for DB Username and Password, A URL we can reach through enviornment variables too.
Request flow looks like: Request from browser ==> MongoExpress/External-Service ==> Mongo Express Pod ==> MongoDB Internal service ==> MongoDB Pod,(Setup using Secret login creds)

For a lot of this, we're writing notes in the configuration files. TBH, kind need to see some port information on the Docker site hosting the container(s)
In the MongoDB container, they've set enviornment variables for the username,(MONGO_INITDB_ROOT_USERNAME), and the password,(MONGO_INITDB_ROOTPASSWORD)

Creating base 64 encrypte passwords for mongo-secret.yaml:
ehco -n 'username'  | base64
           your username   what type of encoding

Note, creation of components matter...you can't reference secrets if they aren't created first!

Secret created: kubectl apply -f mongo-secret.yaml
Then, after secret created and we fixed our mongo config file: kubectl apply -f mongo.yaml

In YAML, you can create multiple 'documents' in one file
In yaml, this, ( --- ) is key for, "New Document)", seen in mongo.yaml
Deployments and Services usually go in one file because they usually go 
together

After creating service, see that it's working:
kubectl get service
AND validate it's connected to the pods:
kubectl describe service mongodb-service
Find all components related to Mongo with Grep:
kubectl get all | grep mongodb

Mongo express creation:
If you have two apps that are using the SAME CREDENTAILS, it might be
smart to create a config map to store that as a centralized 
credentials space for all the apps

Configmap MUST be refferenced in the cluster before refferencing it!
Create ConfigMap first:
kubectl apply -f mongo-configmap.yaml

Then Mongo-express deployment: kubectl apply -f mongo-express.yaml

To see server listening: kubectl logs mongo-express-78fcf796b8-w2vxm

To connect to the mongo-express from the web, we need an external service:

The Load balancer term is tricky...becuase it makes a Service technically external to
applicaitons...but services still load balance INTERNALLY.
Load Balancer value assigns an external IP address for applications

kubectl apply -f mongo-express.yaml
You should be able to see that Load blanacer with:
kubectl get service
When creating service, it's defaulted as 'internal' to the cluster
External has an external IP address and an IP assigned to the port
YOu should see the external IP as pending to begin with. To add it...
minikube service mongo-express-service
You can see it gave a sudo url to access this application from and
a port. Example from our test: http://192.168.49.2:30000

WIth the app open, click 'Create Database'. In practice, what happened was:
Mongo Express External Service was contacted from application ==>
That serive sent the request to the Mongo Express Pod ==>
Mongo DB Internal Service was connected to from Express Pod ==>
Finally, the MongoDB Internal Service forwarded that to the MongoDB Pod

Part 7 Kubernetes NameSpace
* Namespace - Organization for resources. A virtual Cluster inside Kubernetes
Cluster. You get 4 namespaces by default.
Kubectl get namespace. Types of Namespaces:
- kubernetes-dashboard: only with minikube
- kube-system: A system process...DON'T MODIFY!
- kube-public: Publicly accessible data. Accessable without authentification
- kube-node-lease: Contains info on 'heartbeats' of nodes. Determines
availablitly of node
- default: you create new namespace info here...if you haven't already created one

To create namespace:
kubectl create namespace NAME-OF-NameSpace
you can also use a configuartion file for a Namespace

Benefits for Namespace
- Everything is in one space, under one name.
For example, one namespace with a database, with all required resources. Then
another for monitoring, elastic-stack, and application stuff.
According to docs, you don't need to use it for a small project.
- Good for big teams, same application
- Resource sharing, for staging and development. ALso good for versioning production.
- Access and resource limits on Namespaces. So one team only has access to one namespace,
etc. You can also allocate CPU, Ram, Storage, etc for certain teams.

Organization for Namespaces

- You CAN'T access most resources from different Namespaces. For example,
Project A Namespace can't access the configmap from Project Balancer
In the actual YAMl configuration, you can access other resources from other 
Namespaces with a .NAME_SPACE annotator

- Some Components can't be created WITHIN a Namespace...like volumes or nodes.
They live globally within a cluster. To list components NOT bound to a namespace:
kubectl api-resources --namespaced=false
kubectl api-resources --namespaced=true

To create NameSpace: kubectl create namespace <namespace name>
By default, if no Namepsace is given for a component, it defaults to 'default' Namespace
Kubectl actually just substitutes the default namespace if no namespace is given.
The commands actually look like:
kubectl get configmap -n default // -n is namespace, default is default Namepsace
To create a component in a Namespace:
kubectl apply -f mysql-configmap.yaml --namespace=my-namespace
BETTER WAY: Include it in a config file

To make a namespace the default namespace:
There's a tool called kubens you need to install:
Install Brew first, https://brew.sh/
Then: brew install kubectx

With Kubens, you can type:
kubens my-namespace
Now you can execute kubectl commands without providing the namespace

Part 8 Ingress:

Using a service to expose the app is okay for testing...but for a final product, 
we want a domain name! https://my-app.com 
In this case, we do NOT open our app through the IP address and the port service!
The Ingress will instead take teh request from the site, and redirect THAT to the
service. Then that service will redirect it to the pod.

You need an implementation for Ingress...called an Ingress Controller.
It's basically a set of pods that evaulates/processes Ingress rules.
This will be the entrypoint to all request to the Cluster
You have to see which 3rd party controller you like

You must consider the envrionment you're running your cluster on.
For the cloud, you don't need to implement a load balancer, which is nice.
Through not the cloud, you can set up a 'proxy server' which has a public ip 
address and open ports that sort requests as an entrypoint to the cluster.
NO KUBERNETES CLUSTER SHOULD BE ACCESSIBLE EXTERNALLY

To install ingress, (one way, with nGinx): minikube addons enable ingress

Get Kubernete Dashboard: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml

We created an Ingress to access Kubernetes dashboard trhough the host.

NOTE THIS SECTION GOT HARD TO FOLLOW ALONG; SOME STUFF DID NOT COME WITH OUT OF 
THE BOX

She went into /etc/hosts to forward teh host naming. Example:
192.168.64.5	dashboard.com              (This works locally)

Default backend is used to host the kubernetes board if you don't se this 
all up

You can make multiple paths for the same host, (example Google)
In rules, you can define the host, then the paths for each service that
goes to an app, for example

Another good use is sub-domains, like shopping.myapp.com

Configuring TLS Certificate:
Just define TLS above the host, then the secret name,(see video for more)

Data keys for this must be tls.crt and tls.key

Part 9 Helm:

* What is Helm? - Like a package manager,(brew) for Kubernetes. So instead of getting a bunch
of yaml files for configuring one tool in Kubernetes, you bunch them together;
this is called the 'Helm Chart', just a bundle of Yamls.
For example, if you have a deployment you think you need in your cluster, 
you can use helm search KEYWORD to find it. Or you can go to 
Helm Hub.
Helm is a 'Templating Engine'. Using helm you can define a common blueprint for deployment
of Microservices; any values that are different can be changed by placeholders,
in a 'Template' file. These values are definied in those template files.
Helm is also good for deploying across different enviornments

Structure:
mychart/ --name of chart
	chart.yaml -- meta info about chart, (dependencies, etc)
	values.yaml --values for the templates files,(override default values)
	charts/ -- chart dependencies,(does this chart depend on other charts?)
	templates/ -- the actual template files; also, READMEFILE

	...

when chart is installed with helm install CHARTNAME, tempaltes files will
be filled with the values from values.yaml

Difference between Helm V2/V3

Helm V2:
Client(helm CLI) and Server,(Tiller)
Client send commands to Tiller..tiller then executes requests and create components.
It also creates a history of these executions...you can rollback these changes whenever
Downside: There's a LOT of power with the tiller...kind of a security issue...

Helm3:
Removes Tiller because of security, it runs as a binary



Part 10 Volumes:

Volumes must be it's own seperate node; because if a node goes down, other nodes 
need to be able to access it. Storage needs to survive even if the cluster crashed.

Volumes might also be good for a directory storage.

* Persistant Volume - Used for directory data storage. It's it's own cluster resource.
It's created via a YAML file. This needs ACTUAL machine pysical storage. You need to
create and manage these data storage sites yourself...Kubernetes is just a 'plug-in' to 
this cluster. Depending on storage type, (local, cloud, etc), the configuration might 
be a little different. THESE ARE ACCESSABLE TO THE WHOLE CLUSTER - NOT NAMESPACED

* Persistant Volume Claim - Needs to be created when cluster starts...pods need to 
access it! The applications deployed in pods must CLAIM the Persistant Volume, using a 
PVC, (configured in YAML). It also needs to be used in the Pod's configuration

* Storage Class

Volume Types:

Each has it's own use case 

* Local Volume - Does not survive in cluster clash, and is tied to a specific node

* Remote Volume - Is better/safer

Volume extraction example:
A Pod, requests volume through the PVC,(must be in same namespace!) ==>
PVC tries to find a volume in the cluster with enough storage ==>
Volume has the actual storage back-end

Once all that is done, you can create the mount-space INSIDE the actual container 
running in the pod. This is easier for devs because you just worry about 
deploying the applications

You can also mount secret/config maps INTO your containers on your pods, to give them key
data info

Honestly, I think I'll look back into this when I need it...

Part 11 Stateful Set:

* Stateful-set - Used for 'Stateful Applicaitons'. Example, oldDB that stores data 
to keep track of it's state. Stateless applications don't keep record of state. 
So, a simple node.js is stateless...but the mongodb application is stateful. Stateful apps
are hard to maintain

Stateless are deployed with Deployments and can be replicated. 
Stateful Applications are deployed using 'Stateful-Set' deployments.
Storage is configured the same way

Stateful pods can't be created/deleted at same time/randomly addressed.
They are NOT identical - they have their own identity.

Might wanna watch this section later as well...
She basically says, bruh, don't put your DBS in here...

Part 12 Kubernetes Services explained
Different services are used for different use cases.

* Service - Used for a persistant IP Address for Pods. Also allows for load balancing, that way, clients can call for 
a SINGLE IP address instead of the IP of each pod. Services are good for coupling in AND outside the Kluster.

* ClusterIP Services - Default Service Type. Takes Cluster IP if you don't sepcify in the configuration. Cluster IP usually takes 
the Request from the Ingress, (which gets it from the client first), then that service forwards the request INTERNALLY to 
the pods. 

Services can forward requests to pods through 'selectors'; in the configuration of the service, enter 
the key value pair 'selector' towards the pods with their labels.

If two applications are in a pod, the 'targetPort' attribute can be used to forward requests to each application.

When you create a service, Kubernetes creates and 'end-point' object, which is the same name as the service and keeps track of which 
pods/members are apart of the service: kubectl get endpoints

If you want to create a 'mulitport' service, you need to name each port for services to connect to.

* Headless Service - If Client wants to talk to a SPECIFIC pod, without being load balanced...might be used for stateful
applications,(so no usecase for me.... :( )